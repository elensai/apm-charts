---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-mysql/templates/mysqld.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: apm-mysql-mysqldcnf
  namespace: default
data:
  mysqldcnf-config: |-
    # java-hadoop-mysql Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
    #
    # This program is free software; you can redistribute it and/or modify
    # it under the terms of the GNU General Public License as published by
    # the Free Software Foundation; version 2 of the License.
    #
    # This program is distributed in the hope that it will be useful,
    # but WITHOUT ANY WARRANTY; without even the implied warranty of
    # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    # GNU General Public License for more details.
    #
    # You should have received a copy of the GNU General Public License
    # along with this program; if not, write to the Free Software
    # Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA

    #
    # The MySQL  Server configuration file.
    #
    # For explanations see
    # http://dev.mysql.com/doc/mysql/en/server-system-variables.html

    [mysqld]
    pid-file        = /var/run/mysqld/mysqld.pid
    socket          = /var/run/mysqld/mysqld.sock
    datadir         = /var/lib/mysql
    #log-error      = /var/log/mysql/error.log
    # By default we only accept connections from localhost
    #bind-address   = 127.0.0.1
    # Disabling symbolic-links is recommended to prevent assorted security risks
    symbolic-links=0
    character_set_server=utf8mb4
    explicit_defaults_for_timestamp=true
    max_connections = 5000
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-redis/charts/apm-mysql/templates/mysqld.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: apm-mysql-mysqldcnf
  namespace: default
data:
  mysqldcnf-config: |-
    # java-hadoop-redis-mysql Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
    #
    # This program is free software; you can redistribute it and/or modify
    # it under the terms of the GNU General Public License as published by
    # the Free Software Foundation; version 2 of the License.
    #
    # This program is distributed in the hope that it will be useful,
    # but WITHOUT ANY WARRANTY; without even the implied warranty of
    # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    # GNU General Public License for more details.
    #
    # You should have received a copy of the GNU General Public License
    # along with this program; if not, write to the Free Software
    # Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA

    #
    # The MySQL  Server configuration file.
    #
    # For explanations see
    # http://dev.mysql.com/doc/mysql/en/server-system-variables.html

    [mysqld]
    pid-file        = /var/run/mysqld/mysqld.pid
    socket          = /var/run/mysqld/mysqld.sock
    datadir         = /var/lib/mysql
    #log-error      = /var/log/mysql/error.log
    # By default we only accept connections from localhost
    #bind-address   = 127.0.0.1
    # Disabling symbolic-links is recommended to prevent assorted security risks
    symbolic-links=0
    character_set_server=utf8mb4
    explicit_defaults_for_timestamp=true
    max_connections = 5000
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/core-site.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: core-site
  namespace: default
data:
  core-site-config: |-
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <!--
      Licensed under the Apache License, Version 2.0 (the "License");
      you may not use this file except in compliance with the License.
      You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License. See accompanying LICENSE file.
    -->

    <!-- Put site-specific property overrides in this file. -->

    <configuration>

      <property>
            <name>fs.defaultFS</name>
            <value>hdfs://ns</value>
      </property>

      <property>
            <name>hadoop.tmp.dir</name>
            <value>/user/data/hadoop/hdfs/temp</value>
      </property>
      
      <property>    
            <name>io.file.buffer.size</name>    
            <value>65536</value>    
      </property>

      <property>
          <name>ha.zookeeper.quorum</name>
          <value>apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:2181,apm-hadoop-ss-1.ha-svc.default.svc.cluster.local:2181,apm-hadoop-ss-2.ha-svc.default.svc.cluster.local:2181</value>

      </property>

      <property>
          <name>fs.trash.interval</name>
          <value>4320</value>
          <description>Number of minutes between trash checkpoints. If zero, the trash feature is disabled.</description>
      </property>

      <property>
            <name>ipc.client.connect.timeout</name>
            <value>360000</value>
      </property>
      <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
      </property>
      <property>
          <name>hadoop.proxyuser.root.groups</name>
          <value>*</value>
      </property>
    </configuration>
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/hadoop-env.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hadoop-env
  namespace: default
data:
  hadoop-env-config: |-
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    # Set Hadoop-specific environment variables here.

    # The only required environment variable is JAVA_HOME.  All others are
    # optional.  When running a distributed configuration it is best to
    # set JAVA_HOME in this file, so that it is correctly defined on
    # remote nodes.

    # The java implementation to use.
    export JAVA_HOME=/user/apps/jdk1.8.0_131

    # The jsvc implementation to use. Jsvc is required to run secure datanodes
    # that bind to privileged ports to provide authentication of data transfer
    # protocol.  Jsvc is not required if SASL is configured for authentication of
    # data transfer protocol using non-privileged ports.
    #export JSVC_HOME=${JSVC_HOME}

    export HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-"/etc/hadoop"}

    # Extra Java CLASSPATH elements.  Automatically insert capacity-scheduler.
    #for f in $HADOOP_HOME/contrib/capacity-scheduler/*.jar; do
    #  if [ "$HADOOP_CLASSPATH" ]; then
    #    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f
    #  else
    #    export HADOOP_CLASSPATH=$f
    #  fi
    #done

    # The maximum amount of heap to use, in MB. Default is 1000.
    #export HADOOP_HEAPSIZE=
    #export HADOOP_NAMENODE_INIT_HEAPSIZE=""

    # Extra Java runtime options.  Empty by default.
    export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"

    # Command specific options appended to HADOOP_OPTS when specified
    export HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_NAMENODE_OPTS"
    export HADOOP_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS"

    export HADOOP_SECONDARYNAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender} $HADOOP_SECONDARYNAMENODE_OPTS"

    export HADOOP_NFS3_OPTS="$HADOOP_NFS3_OPTS"
    export HADOOP_PORTMAP_OPTS="-Xmx512m $HADOOP_PORTMAP_OPTS"

    # The following applies to multiple commands (fs, dfs, fsck, distcp etc)
    export HADOOP_CLIENT_OPTS="-Xmx512m $HADOOP_CLIENT_OPTS"
    #HADOOP_JAVA_PLATFORM_OPTS="-XX:-UsePerfData $HADOOP_JAVA_PLATFORM_OPTS"

    # On secure datanodes, user to run the datanode as after dropping privileges.
    # This **MUST** be uncommented to enable secure HDFS if using privileged ports
    # to provide authentication of data transfer protocol.  This **MUST NOT** be
    # defined if SASL is configured for authentication of data transfer protocol
    # using non-privileged ports.
    export HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER}

    # Where log files are stored.  $HADOOP_HOME/logs by default.
    #export HADOOP_LOG_DIR=${HADOOP_LOG_DIR}/$USER

    # Where log files are stored in the secure data environment.
    export HADOOP_SECURE_DN_LOG_DIR=${HADOOP_LOG_DIR}/${HADOOP_HDFS_USER}

    ###
    # HDFS Mover specific parameters
    ###
    # Specify the JVM options to be used when starting the HDFS Mover.
    # These options will be appended to the options specified as HADOOP_OPTS
    # and therefore may override any similar flags set in HADOOP_OPTS
    #
    # export HADOOP_MOVER_OPTS=""

    ###
    # Advanced Users Only!
    ###

    # The directory where pid files are stored. /tmp by default.
    # NOTE: this should be set to a directory that can only be written to by 
    #       the user that will run the hadoop daemons.  Otherwise there is the
    #       potential for a symlink attack.
    export HADOOP_PID_DIR=${HADOOP_PID_DIR}
    export HADOOP_SECURE_DN_PID_DIR=${HADOOP_PID_DIR}

    # A string representing this instance of hadoop. $USER by default.
    export HADOOP_IDENT_STRING=$USER
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/hbase-env.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hbase-env
  namespace: default
data:
  hbase-env-config: |-
    #
    #/**
    # * Licensed to the Apache Software Foundation (ASF) under one
    # * or more contributor license agreements.  See the NOTICE file
    # * distributed with this work for additional information
    # * regarding copyright ownership.  The ASF licenses this file
    # * to you under the Apache License, Version 2.0 (the
    # * "License"); you may not use this file except in compliance
    # * with the License.  You may obtain a copy of the License at
    # *
    # *     http://www.apache.org/licenses/LICENSE-2.0
    # *
    # * Unless required by applicable law or agreed to in writing, software
    # * distributed under the License is distributed on an "AS IS" BASIS,
    # * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # * See the License for the specific language governing permissions and
    # * limitations under the License.
    # */

    # Set environment variables here.

    # This script sets variables multiple times over the course of starting an hbase process,
    # so try to keep things idempotent unless you want to take an even deeper look
    # into the startup scripts (bin/hbase, etc.)

    # The java implementation to use.  Java 1.7+ required.
    export JAVA_HOME=/user/apps/jdk1.8.0_131/

    # Extra Java CLASSPATH elements.  Optional.
    # export HBASE_CLASSPATH=

    # The maximum amount of heap to use. Default is left to JVM default.
    # export HBASE_HEAPSIZE=1G

    # Uncomment below if you intend to use off heap cache. For example, to allocate 8G of 
    # offheap, set the value to "8G".
    # export HBASE_OFFHEAPSIZE=1G

    # Extra Java runtime options.
    # Below are what we set by default.  May only work with SUN JVM.
    # For more on why as well as other possible settings,
    # see http://wiki.apache.org/hadoop/PerformanceTuning
    export HBASE_OPTS="-XX:+UseConcMarkSweepGC"

    # Configure PermSize. Only needed in JDK7. You can safely remove it for JDK8+
    export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m"
    export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m -XX:ReservedCodeCacheSize=256m"

    # Uncomment one of the below three options to enable java garbage collection logging for the server-side processes.

    # This enables basic gc logging to the .out file.
    # export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"

    # This enables basic gc logging to its own file.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH>"

    # This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export SERVER_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH> -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"

    # Uncomment one of the below three options to enable java garbage collection logging for the client processes.

    # This enables basic gc logging to the .out file.
    # export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps"

    # This enables basic gc logging to its own file.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH>"

    # This enables basic GC logging to its own file with automatic log rolling. Only applies to jdk 1.6.0_34+ and 1.7.0_2+.
    # If FILE-PATH is not replaced, the log file(.gc) would still be generated in the HBASE_LOG_DIR .
    # export CLIENT_GC_OPTS="-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:<FILE-PATH> -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=1 -XX:GCLogFileSize=512M"

    # See the package documentation for org.apache.hadoop.hbase.io.hfile for other configurations
    # needed setting up off-heap block caching. 

    # Uncomment and adjust to enable JMX exporting
    # See jmxremote.password and jmxremote.access in $JRE_HOME/lib/management to configure remote password access.
    # More details at: http://java.sun.com/javase/6/docs/technotes/guides/management/agent.html
    # NOTE: HBase provides an alternative JMX implementation to fix the random ports issue, please see JMX
    # section in HBase Reference Guide for instructions.

    # export HBASE_JMX_BASE="-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false"
    # export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10101"
    # export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10102"
    # export HBASE_THRIFT_OPTS="$HBASE_THRIFT_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10103"
    # export HBASE_ZOOKEEPER_OPTS="$HBASE_ZOOKEEPER_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10104"
    # export HBASE_REST_OPTS="$HBASE_REST_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10105"

    # File naming hosts on which HRegionServers will run.  $HBASE_HOME/conf/regionservers by default.
    # export HBASE_REGIONSERVERS=${HBASE_HOME}/conf/regionservers

    # Uncomment and adjust to keep all the Region Server pages mapped to be memory resident
    #HBASE_REGIONSERVER_MLOCK=true
    #HBASE_REGIONSERVER_UID="hbase"

    # File naming hosts on which backup HMaster will run.  $HBASE_HOME/conf/backup-masters by default.
    # export HBASE_BACKUP_MASTERS=${HBASE_HOME}/conf/backup-masters

    # Extra ssh options.  Empty by default.
    # export HBASE_SSH_OPTS="-o ConnectTimeout=1 -o SendEnv=HBASE_CONF_DIR"

    # Where log files are stored.  $HBASE_HOME/logs by default.
    # export HBASE_LOG_DIR=${HBASE_HOME}/logs

    # Enable remote JDWP debugging of major HBase processes. Meant for Core Developers 
    # export HBASE_MASTER_OPTS="$HBASE_MASTER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8070"
    # export HBASE_REGIONSERVER_OPTS="$HBASE_REGIONSERVER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8071"
    # export HBASE_THRIFT_OPTS="$HBASE_THRIFT_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8072"
    # export HBASE_ZOOKEEPER_OPTS="$HBASE_ZOOKEEPER_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8073"

    # A string representing this instance of hbase. $USER by default.
    # export HBASE_IDENT_STRING=$USER

    # The scheduling priority for daemon processes.  See 'man nice'.
    # export HBASE_NICENESS=10

    # The directory where pid files are stored. /tmp by default.
    # export HBASE_PID_DIR=/var/hadoop/pids

    # Seconds to sleep between slave commands.  Unset by default.  This
    # can be useful in large clusters, where, e.g., slave rsyncs can
    # otherwise arrive faster than the master can service them.
    # export HBASE_SLAVE_SLEEP=0.1

    # Tell HBase whether it should manage it's own instance of Zookeeper or not.
    export HBASE_MANAGES_ZK=false

    # The default log rolling policy is RFA, where the log file is rolled as per the size defined for the 
    # RFA appender. Please refer to the log4j.properties file to see more details on this appender.
    # In case one needs to do log rolling on a date change, one should set the environment property
    # HBASE_ROOT_LOGGER to "<DESIRED_LOG LEVEL>,DRFA".
    # For example:
    # HBASE_ROOT_LOGGER=INFO,DRFA
    # The reason for changing default to RFA is to avoid the boundary case of filling out disk space as 
    # DRFA doesn't put any cap on the log size. Please refer to HBase-5655 for more context.
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/hbase-site.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hbase-site
  namespace: default
data:
  hbase-site-config: |-
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <!--
    /**
    *
    * Licensed to the Apache Software Foundation (ASF) under one
    * or more contributor license agreements.  See the NOTICE file
    * distributed with this work for additional information
    * regarding copyright ownership.  The ASF licenses this file
    * to you under the Apache License, Version 2.0 (the
    * "License"); you may not use this file except in compliance
    * with the License.  You may obtain a copy of the License at
    *
    *     http://www.apache.org/licenses/LICENSE-2.0
    *
    * Unless required by applicable law or agreed to in writing, software
    * distributed under the License is distributed on an "AS IS" BASIS,
    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    * See the License for the specific language governing permissions and
    * limitations under the License.
    */
    -->
    <configuration>
      <property>
        <name>hbase.rootdir</name>
        <value>hdfs://ns/hbase</value>
      </property>

      <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
      </property>

      <property>
        <name>hbase.zookeeper.quorum</name>
          <value>apm-hadoop-ss-0.ha-svc.default.svc.cluster.local,apm-hadoop-ss-1.ha-svc.default.svc.cluster.local,apm-hadoop-ss-2.ha-svc.default.svc.cluster.local</value>
      </property>
    </configuration>
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/hdfs-site.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hdfs-site
  namespace: default
data:
  hdfs-site-config: |-
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <!--
      Licensed under the Apache License, Version 2.0 (the "License");
      you may not use this file except in compliance with the License.
      You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License. See accompanying LICENSE file.
    -->

    <!-- Put site-specific property overrides in this file. -->

    <configuration>
      <property>    
        <name>dfs.nameservices</name>    
        <value>ns</value> 
      </property>  

        <property>
              <name>dfs.ha.namenodes.ns</name>
              <value>nn1,nn2</value> 
        </property>
      

      <property>
              <name>dfs.namenode.rpc-address.ns.nn1</name>
              <value>apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:9000</value>
      </property>
      
        <property>
              <name>dfs.namenode.http-address.ns.nn1</name>
              <value>apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:50070</value>
        </property>

      
        <property>
              <name>dfs.namenode.rpc-address.ns.nn2</name>
              <value>apm-hadoop-ss-2.ha-svc.default.svc.cluster.local:9000</value>
        </property>
            
        <property>
              <name>dfs.namenode.http-address.ns.nn2</name>
              <value>apm-hadoop-ss-2.ha-svc.default.svc.cluster.local:50070</value>
        </property>


        <property>
                <name>dfs.namenode.shared.edits.dir</name>
                <value>qjournal://apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:8485;apm-hadoop-ss-1.ha-svc.default.svc.cluster.local:8485;apm-hadoop-ss-2.ha-svc.default.svc.cluster.local:8485/ns</value>

        </property>

        <property>
                <name>dfs.journalnode.edits.dir</name>
                <value>/user/data/hadoop/journal</value>
        </property>

        <property>
              <name>dfs.ha.automatic-failover.enabled</name>
              <value>true</value>
        </property>

        <property>
              <name>dfs.client.failover.proxy.provider.ns</name>
              <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>


        <property>
              <name>dfs.ha.fencing.methods</name>
              <value>sshfence</value>
        </property>
      
        <property>
              <name>dfs.ha.fencing.ssh.private-key-files</name>
              <value>/root/.ssh/id_rsa</value>
        </property>

        <property>    
              <name>dfs.namenode.name.dir</name>    
              <value>file:///user/data/hadoop/hdfs/name</value>
        </property>    
      
        <property>    
            <name>dfs.datanode.data.dir</name>    
            <value>file:///user/data/hadoop/hdfs/data</value> 
        </property> 

        <property>    
              <name>dfs.replication</name>    
              <value>3</value>    
        </property>

        <property>    
            <name>dfs.webhdfs.enabled</name>    
            <value>true</value>    
        </property>    
        
    </configuration>
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/hive-env.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hive-env
  namespace: default
data:
  hive-env-config: |-
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    # Set Hive and Hadoop environment variables here. These variables can be used
    # to control the execution of Hive. It should be used by admins to configure
    # the Hive installation (so that users do not have to set environment variables
    # or set command line parameters to get correct behavior).
    #
    # The hive service being invoked (CLI/HWI etc.) is available via the environment
    # variable SERVICE


    # Hive Client memory usage can be an issue if a large number of clients
    # are running at the same time. The flags below have been useful in 
    # reducing memory usage:
    #
    # if [ "$SERVICE" = "cli" ]; then
    #   if [ -z "$DEBUG" ]; then
    #     export HADOOP_OPTS="$HADOOP_OPTS -XX:NewRatio=12 -Xms10m -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:+UseParNewGC -XX:-UseGCOverheadLimit"
    #   else
    #     export HADOOP_OPTS="$HADOOP_OPTS -XX:NewRatio=12 -Xms10m -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:-UseGCOverheadLimit"
    #   fi
    # fi

    # The heap size of the jvm stared by hive shell script can be controlled via:
    #
    # export HADOOP_HEAPSIZE=1024
    #
    # Larger heap size may be required when running queries over large number of files or partitions. 
    # By default hive shell scripts use a heap size of 256 (MB).  Larger heap size would also be 
    # appropriate for hive server (hwi etc).


    # Set HADOOP_HOME to point to a specific hadoop install directory
    # HADOOP_HOME=${bin}/../../hadoop

    # Hive Configuration Directory can be controlled by:
    # export HIVE_CONF_DIR=

    # Folder containing extra ibraries required for hive compilation/execution can be controlled by:
    # export HIVE_AUX_JARS_PATH=


    # export JAVA_HOME=/user/apps/jdk1.8.0_131
    # export HADOOP_HOME=/user/apps/hadoop-2.7.0
    # export HIVE_HOME=/user/apps/hive-1.2.1/
    # export HIVE_CONF_DIR=/user/apps/hive-1.2.1/conf
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/hive-log4j.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hive-log4j
  namespace: default
data:
  hive-log4j-config: |-
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #     http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.

    # Define some default values that can be overridden by system properties
    hive.log.threshold=ALL
    hive.root.logger=INFO,DRFA
    hive.log.dir=/user/apps/hive-1.2.1/logs
    hive.log.file=hive.log

    # Define the root logger to the system property "hadoop.root.logger".
    log4j.rootLogger=${hive.root.logger}, EventCounter

    # Logging Threshold
    log4j.threshold=${hive.log.threshold}

    #
    # Daily Rolling File Appender
    #
    # Use the PidDailyerRollingFileAppend class instead if you want to use separate log files
    # for different CLI session.
    #
    # log4j.appender.DRFA=org.apache.hadoop.hive.ql.log.PidDailyRollingFileAppender

    log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender

    log4j.appender.DRFA.File=${hive.log.dir}/${hive.log.file}

    # Rollver at midnight
    log4j.appender.DRFA.DatePattern=.yyyy-MM-dd

    # 30-day backup
    #log4j.appender.DRFA.MaxBackupIndex=30
    log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout

    # Pattern format: Date LogLevel LoggerName LogMessage
    #log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
    # Debugging Pattern format
    log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p [%t]: %c{2} (%F:%M(%L)) - %m%n


    #
    # console
    # Add "console" to rootlogger above if you want to use this
    #

    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.target=System.err
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} [%t]: %p %c{2}: %m%n
    log4j.appender.console.encoding=UTF-8

    #custom logging levels
    #log4j.logger.xxx=DEBUG

    #
    # Event Counter Appender
    # Sends counts of logging messages at different severity levels to Hadoop Metrics.
    #
    #log4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter

    log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter

    log4j.category.DataNucleus=ERROR,DRFA
    log4j.category.Datastore=ERROR,DRFA
    log4j.category.Datastore.Schema=ERROR,DRFA
    log4j.category.JPOX.Datastore=ERROR,DRFA
    log4j.category.JPOX.Plugin=ERROR,DRFA
    log4j.category.JPOX.MetaData=ERROR,DRFA
    log4j.category.JPOX.Query=ERROR,DRFA
    log4j.category.JPOX.General=ERROR,DRFA
    log4j.category.JPOX.Enhancer=ERROR,DRFA


    # Silence useless ZK logs
    log4j.logger.org.apache.zookeeper.server.NIOServerCnxn=WARN,DRFA
    log4j.logger.org.apache.zookeeper.ClientCnxnSocketNIO=WARN,DRFA
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/hive-site.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: hive-site
  namespace: default
data:
  hive-site-config: |-
    <?xml version="1.0" encoding="UTF-8" standalone="no"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?><!--
      Licensed to the Apache Software Foundation (ASF) under one or more
      contributor license agreements.  See the NOTICE file distributed with
      this work for additional information regarding copyright ownership.
      The ASF licenses this file to You under the Apache License, Version 2.0
      (the "License"); you may not use this file except in compliance with
      the License.  You may obtain a copy of the License at

          http://www.apache.org/licenses/LICENSE-2.0

      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License.
    -->

    <configuration>
      <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://apm-mysql-svc:3306/hivedb?createDatabaseIfNotExist=true&amp;useSSL=false</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
      </property>
      <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>pw123456</value>
      </property>
    </configuration>
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/init-sh.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: initpsh
  namespace: default
data:
  initpsh-config: |-
    #!/bin/bash
    
    host_dns_0="apm-hadoop-ss-0.ha-svc.default.svc.cluster.local"
    host_dns_1="apm-hadoop-ss-1.ha-svc.default.svc.cluster.local"
    host_dns_2="apm-hadoop-ss-2.ha-svc.default.svc.cluster.local"
    touch /user/${host_num}f.txt

    volumeinitvar=true
    echo "是否初始化大数据平台："$volumeinitvar
    

    echo "开始安装大数据平台"
     
    until [ `ping -c 1 apm-hadoop-ss-0.ha-svc.default.svc.cluster.local | grep ', 0% packet loss,' | wc -l` = 1  -a   `ping -c 1 apm-hadoop-ss-1.ha-svc.default.svc.cluster.local | grep ', 0% packet loss,' | wc -l` = 1  -a   `ping -c 1 apm-hadoop-ss-2.ha-svc.default.svc.cluster.local | grep ', 0% packet loss,' | wc -l` = 1   ]
    do
      sleep 3
      echo "等待DNS服务"
    done

    if [ $volumeinitvar = true ]; then
      echo "清理源数据"
      ssh root@$host_dns_0 "rm -rf /user/data/hadoop/*"
      ssh root@$host_dns_1 "rm -rf /user/data/hadoop/*"
      ssh root@$host_dns_2 "rm -rf /user/data/hadoop/*"
    fi

    
    ssh root@$host_dns_0 "source /etc/profile && zkServer.sh start"
    ssh root@$host_dns_1 "source /etc/profile && zkServer.sh start"
    ssh root@$host_dns_2 "source /etc/profile && zkServer.sh start"

    ssh root@$host_dns_0 "source /etc/profile && hadoop-daemons.sh start journalnode"      
    

    if [ $volumeinitvar = true ]; then
      echo "初始化hdfs"
      ssh root@$host_dns_0 "source /etc/profile && hdfs zkfc -formatZK"
      ssh root@$host_dns_0 "source /etc/profile && hadoop namenode -format"
      ssh root@$host_dns_2 "scp -r apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:/user/data/hadoop/hdfs /user/data/hadoop/"
    else
      ssh root@$host_dns_0 "source /etc/profile && hdfs zkfc -formatZK"
    fi

    ssh root@$host_dns_0 "source /etc/profile && start-dfs.sh"
    ssh root@$host_dns_1 "source /etc/profile && start-yarn.sh"
    ssh root@$host_dns_2 "source /etc/profile && yarn-daemon.sh start resourcemanager"
    ssh root@$host_dns_0 "source /etc/profile && mr-jobhistory-daemon.sh start historyserver"
    


    echo "初始化hive"
    ssh root@$host_dns_1 "source /etc/profile && echo \"nohup hive --service hiveserver2 &\" > /user/a.sh && nohup /user/a.sh >/dev/null 2>&1 &"
    sleep 20

    echo "初始化hbase"
    ssh root@$host_dns_1 "source /etc/profile && start-hbase.sh"
    sleep 10

    echo "初始化oozie"
    ssh root@$host_dns_0 "source /etc/profile && oozie-setup.sh prepare-war sharelib create -fs hdfs://ns  -locallib  /user/apps/oozie-4.0.0/oozie-sharelib-4.0.0-cdh5.3.6-yarn.tar.gz"
    ssh root@$host_dns_0 "source /etc/profile && ooziedb.sh create -sqlfile oozie.sql -run DB Connection /user/apps/oozie-4.0.0/"
    ssh root@$host_dns_0 "source /etc/profile && oozied.sh start"
    
    if [ $volumeinitvar = true ]; then
      echo "初始化算法组件"
      ssh root@$host_dns_0 "source /etc/profile && hadoop dfs -mkdir /EML/ && hadoop dfs -mkdir /EML/lensData && hadoop dfs -mkdir /EML/LensProcessData/ && hadoop dfs -mkdir /EML/oozie/ && hadoop dfs -mkdir /EML/tmp/ && cd /user/apps/data-modules && hadoop dfs -put -f Modules /EML/"
    fi

    # touch /initdata/o.lock;
    echo "大数据平台启动成功！"
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/mapred-site.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: mapred-site
  namespace: default
data:
  mapred-site-config: |-
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <!--
      Licensed under the Apache License, Version 2.0 (the "License");
      you may not use this file except in compliance with the License.
      You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License. See accompanying LICENSE file.
    -->

    <!-- Put site-specific property overrides in this file. -->

    <configuration>
      <property>    
            <name>mapreduce.framework.name</name>    
            <value>yarn</value>    
      </property>  

      <property>  
            <name>mapreduce.jobhistory.address</name>  
            <value>apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:10020</value>  
      </property> 


      <property>  
          <name>mapreduce.jobhistory.webapp.address</name>
          <value>apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:19888</value>  
      </property>


      <property>  
          <name>mapreduce.jobhistory.intermediate-done-dir</name>
          <value>/usr/hadoop/tmp/mr_history</value> 
      </property>  

      <property>  
          <name>mapreduce.jobhistory.done-dir</name>  
          <value>/usr/hadoop/tmp/mr_history</value> 
      </property>

      <property>
            <name>mapreduce.reduce.shuffle.input.buffer.percent</name>
            <value>0.6</value>
      </property>

        <property>
            <name>mapreduce.reduce.shuffle.memory.limit.percent</name>
            <value>0.10</value> 
      </property>


    </configuration>
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/oozie-site.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: oozie-site
  namespace: default
data:
  oozie-site-config: |-
    <?xml version="1.0"?>
    <!--
    Licensed to the Apache Software Foundation (ASF) under one
    or more contributor license agreements.  See the NOTICE file
    distributed with this work for additional information
    regarding copyright ownership.  The ASF licenses this file
    to you under the Apache License, Version 2.0 (the
    "License"); you may not use this file except in compliance
    with the License.  You may obtain a copy of the License at
    
        http://www.apache.org/licenses/LICENSE-2.0
    
    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
    -->
    <configuration>
        <!--
            Refer to the oozie-default.xml file for the complete list of
            Oozie configuration properties and their default values.
        -->
        <property>
            <name>oozie.service.ActionService.executor.ext.classes</name>
            <value>
                org.apache.oozie.action.email.EmailActionExecutor,
                org.apache.oozie.action.hadoop.HiveActionExecutor,
                org.apache.oozie.action.hadoop.ShellActionExecutor,
                org.apache.oozie.action.hadoop.SqoopActionExecutor,
                org.apache.oozie.action.hadoop.DistcpActionExecutor,
                org.apache.oozie.action.hadoop.Hive2ActionExecutor
            </value>
        </property>

        <property>
            <name>oozie.service.SchemaService.wf.ext.schemas</name>
            <value>
                shell-action-0.1.xsd,shell-action-0.2.xsd,shell-action-0.3.xsd,email-action-0.1.xsd,email-action-0.2.xsd,
                hive-action-0.2.xsd,hive-action-0.3.xsd,hive-action-0.4.xsd,hive-action-0.5.xsd,sqoop-action-0.2.xsd,
                sqoop-action-0.3.xsd,sqoop-action-0.4.xsd,ssh-action-0.1.xsd,ssh-action-0.2.xsd,distcp-action-0.1.xsd,
                distcp-action-0.2.xsd,oozie-sla-0.1.xsd,oozie-sla-0.2.xsd,hive2-action-0.1.xsd
            </value>
        </property>

        <property>
            <name>oozie.system.id</name>
            <value>oozie-${user.name}</value>
            <description>
                The Oozie system ID.
            </description>
        </property>

        <property>
            <name>oozie.systemmode</name>
            <value>NORMAL</value>
            <description>
                System mode for  Oozie at startup.
            </description>
        </property>

        <property>
            <name>oozie.service.AuthorizationService.security.enabled</name>
            <value>false</value>
            <description>
                Specifies whether security (user name/admin role) is enabled or not.
                If disabled any user can manage Oozie system and manage any job.
            </description>
        </property>

        <property>
            <name>oozie.service.PurgeService.older.than</name>
            <value>30</value>
            <description>
                Jobs older than this value, in days, will be purged by the PurgeService.
            </description>
        </property>

        <property>
            <name>oozie.service.HadoopAccessorService.jobTracker.whitelist</name>
            <value>apm-hadoop-ss-1.ha-svc.default.svc.cluster.local:8032</value>
        </property>
        <property>
            <name>oozie.service.HadoopAccessorService.jobTracker.whitelist</name>
            <value>apm-hadoop-ss-2.ha-svc.default.svc.cluster.local:8032</value>
        </property>

        <property>
            <name>oozie.service.PurgeService.purge.interval</name>
            <value>3600</value>
            <description>
                Interval at which the purge service will run, in seconds.
            </description>
        </property>

        <property>
            <name>oozie.service.CallableQueueService.queue.size</name>
            <value>10000</value>
            <description>Max callable queue size</description>
        </property>

        <property>
            <name>oozie.service.CallableQueueService.threads</name>
            <value>10</value>
            <description>Number of threads used for executing callables</description>
        </property>

        <property>
            <name>oozie.service.CallableQueueService.callable.concurrency</name>
            <value>3</value>
            <description>
                Maximum concurrency for a given callable type.
                Each command is a callable type (submit, start, run, signal, job, jobs, suspend,resume, etc).
                Each action type is a callable type (Map-Reduce, Pig, SSH, FS, sub-workflow, etc).
                All commands that use action executors (action-start, action-end, action-kill and action-check) use
                the action type as the callable type.
            </description>
        </property>

        <property>
            <name>oozie.service.coord.normal.default.timeout
            </name>
            <value>120</value>
            <description>Default timeout for a coordinator action input check (in minutes) for normal job.
                -1 means infinite timeout</description>
        </property>

        <property>
            <name>oozie.db.schema.name</name>
            <value>oozie</value>
            <description>
                Oozie DataBase Name
            </description>
        </property>

        <property>
            <name>oozie.service.JPAService.create.db.schema</name>
            <value>false</value>
            <description>
                Creates Oozie DB.

                If set to true, it creates the DB schema if it does not exist. If the DB schema exists is a NOP.
                If set to false, it does not create the DB schema. If the DB schema does not exist it fails start up.
            </description>
        </property>

        <property>
            <name>oozie.service.JPAService.jdbc.driver</name>
            <value>com.mysql.jdbc.Driver</value>
            <description>
                JDBC driver class.
            </description>
        </property>

        <property>
            <name>oozie.service.JPAService.jdbc.url</name>
            <value>jdbc:mysql://apm-mysql-svc:3306/oozie?createDatabaseIfNotExist=true</value>
            <description>
                JDBC URL.
            </description>
        </property>

        <property>
            <name>oozie.service.JPAService.jdbc.username</name>
            <value>root</value>
            <description>
                DB user name.
            </description>
        </property>

        <property>
            <name>oozie.service.JPAService.jdbc.password</name>
            <value>pw123456</value>
            <description>
                DB user password.

                IMPORTANT: if password is emtpy leave a 1 space string, the service trims the value,
                        if empty Configuration assumes it is NULL.
            </description>
        </property>

        <property>
            <name>oozie.service.JPAService.pool.max.active.conn</name>
            <value>10</value>
            <description>
                Max number of connections.
            </description>
        </property>

        <property>
            <name>oozie.service.HadoopAccessorService.kerberos.enabled</name>
            <value>false</value>
            <description>
                Indicates if Oozie is configured to use Kerberos.
            </description>
        </property>

        <property>
            <name>local.realm</name>
            <value>LOCALHOST</value>
            <description>
                Kerberos Realm used by Oozie and Hadoop. Using 'local.realm' to be aligned with Hadoop configuration
            </description>
        </property>

        <property>
            <name>oozie.service.HadoopAccessorService.keytab.file</name>
            <value>${user.home}/oozie.keytab</value>
            <description>
                Location of the Oozie user keytab file.
            </description>
        </property>

        <property>
            <name>oozie.service.HadoopAccessorService.kerberos.principal</name>
            <value>${user.name}/localhost@${local.realm}</value>
            <description>
                Kerberos principal for Oozie service.
            </description>
        </property>

        <property>
            <name>oozie.service.HadoopAccessorService.jobTracker.whitelist</name>
            <value> </value>
            <description>
                Whitelisted job tracker for Oozie service.
            </description>
        </property>

        <property>
            <name>oozie.service.HadoopAccessorService.nameNode.whitelist</name>
            <value> </value>
            <description>
                Whitelisted job tracker for Oozie service.
            </description>
        </property>

        <property>
            <name>oozie.service.HadoopAccessorService.hadoop.configurations</name>
            <value>*=/user/apps/hadoop-2.7.1/etc/hadoop</value>
            <description>
                Comma separated AUTHORITY=HADOOP_CONF_DIR, where AUTHORITY is the HOST:PORT of
                the Hadoop service (JobTracker, HDFS). The wildcard '*' configuration is
                used when there is no exact match for an authority. The HADOOP_CONF_DIR contains
                the relevant Hadoop *-site.xml files. If the path is relative is looked within
                the Oozie configuration directory; though the path can be absolute (i.e. to point
                to Hadoop client conf/ directories in the local filesystem.
            </description>
        </property>

        <property>
            <name>oozie.service.WorkflowAppService.system.libpath</name>
            <value>/user/${user.name}/share/lib</value>
            <description>
                System library path to use for workflow applications.
                This path is added to workflow application if their job properties sets
                the property 'oozie.use.system.libpath' to true.
            </description>
        </property>

        <property>
            <name>use.system.libpath.for.mapreduce.and.pig.jobs</name>
            <value>false</value>
            <description>
                If set to true, submissions of MapReduce and Pig jobs will include
                automatically the system library path, thus not requiring users to
                specify where the Pig JAR files are. Instead, the ones from the system
                library path are used.
            </description>
        </property>

        <property>
            <name>oozie.authentication.type</name>
            <value>simple</value>
            <description>
                Defines authentication used for Oozie HTTP endpoint.
                Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#
            </description>
        </property>

        <property>
            <name>oozie.authentication.token.validity</name>
            <value>36000</value>
            <description>
                Indicates how long (in seconds) an authentication token is valid before it has
                to be renewed.
            </description>
        </property>

        <property>
        <name>oozie.authentication.cookie.domain</name>
        <value></value>
        <description>
            The domain to use for the HTTP cookie that stores the authentication token.
            In order to authentiation to work correctly across multiple hosts
            the domain must be correctly set.
        </description>
        </property>

        <property>
            <name>oozie.authentication.simple.anonymous.allowed</name>
            <value>true</value>
            <description>
                Indicates if anonymous requests are allowed.
                This setting is meaningful only when using 'simple' authentication.
            </description>
        </property>

        <property>
            <name>oozie.authentication.kerberos.principal</name>
            <value>HTTP/localhost@${local.realm}</value>
            <description>
                Indicates the Kerberos principal to be used for HTTP endpoint.
                The principal MUST start with 'HTTP/' as per Kerberos HTTP SPNEGO specification.
            </description>
        </property>

        <property>
            <name>oozie.authentication.kerberos.keytab</name>
            <value>${oozie.service.HadoopAccessorService.keytab.file}</value>
            <description>
                Location of the keytab file with the credentials for the principal.
                Referring to the same keytab file Oozie uses for its Kerberos credentials for Hadoop.
            </description>
        </property>

        <property>
            <name>oozie.authentication.kerberos.name.rules</name>
            <value>DEFAULT</value>
            <description>
                The kerberos names rules is to resolve kerberos principal names, refer to Hadoop's
                KerberosName for more details.
            </description>
        </property>

        <!-- Proxyuser Configuration -->

        <!--

        <property>
            <name>oozie.service.ProxyUserService.proxyuser.#USER#.hosts</name>
            <value>*</value>
            <description>
                List of hosts the '#USER#' user is allowed to perform 'doAs'
                operations.

                The '#USER#' must be replaced with the username o the user who is
                allowed to perform 'doAs' operations.

                The value can be the '*' wildcard or a list of hostnames.

                For multiple users copy this property and replace the user name
                in the property name.
            </description>
        </property>

        <property>
            <name>oozie.service.ProxyUserService.proxyuser.#USER#.groups</name>
            <value>*</value>
            <description>
                List of groups the '#USER#' user is allowed to impersonate users
                from to perform 'doAs' operations.

                The '#USER#' must be replaced with the username o the user who is
                allowed to perform 'doAs' operations.

                The value can be the '*' wildcard or a list of groups.

                For multiple users copy this property and replace the user name
                in the property name.
            </description>
        </property>

        -->

        <!-- Default proxyuser configuration for Hue -->

        <property>
            <name>oozie.service.ProxyUserService.proxyuser.hue.hosts</name>
            <value>*</value>
        </property>

        <property>
            <name>oozie.service.ProxyUserService.proxyuser.hue.groups</name>
            <value>*</value>
        </property>

    </configuration>
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/sh.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: initsh
  namespace: default
data:
  initsh-config: |-
    #!/bin/bash
    export JAVA_HOME=/user/apps/jdk1.8.0_131
    export SCALA_HOME=/user/apps/scala-2.11.8
    export ZOOKEEPER_HOME=/user/apps/zookeeper-3.4.12
    export HADOOP_HOME=/user/apps/hadoop-2.7.1
    export HIVE_HOME=/user/apps/hive-1.2.1
    export HBASE_HOME=/user/apps/hbase-1.4.4
    export SPARK_HOME=/user/apps/spark-2.3.4
    export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/jre/lib/rt.jar
    export HCAT_HOME=$HIVE_HOME/hcatalog
    export KYLIN_HOME=/user/apps/kylin-1.6.0
    export CATALINA_HOME=$KYLIN_HOME/tomcat
    export hive_dependency=$HIVE_HOME/conf:$HIVE_HOME/lib/*:$HIVE_HOME/share/hcatalog/hive-hcatalog-core-1.2.1.jar
    export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$SCALA_HOME/bin:$ZOOKEEPER_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$HBASE_HOME/bin:$SPARK_HOME/bin:$PATH:$HCAT_HOME/bin:$KYLIN_HOME/bin:$HOME/bin
    host_num=`echo $HOSTNAME | awk -F- '{print $NF}'`
    echo $host_num > /user/data/zookeeper/data/myid 
    exec /usr/sbin/init
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/slaves.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: slaves
  namespace: default
data:
  slaves-config: |-
    apm-hadoop-ss-0.ha-svc.default.svc.cluster.local
    apm-hadoop-ss-1.ha-svc.default.svc.cluster.local
    apm-hadoop-ss-2.ha-svc.default.svc.cluster.local
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/spark-env.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: sparkenv
  namespace: default
data:
  sparkenv-config: |-
    #!/usr/bin/env bash

    #
    # Licensed to the Apache Software Foundation (ASF) under one or more
    # contributor license agreements.  See the NOTICE file distributed with
    # this work for additional information regarding copyright ownership.
    # The ASF licenses this file to You under the Apache License, Version 2.0
    # (the "License"); you may not use this file except in compliance with
    # the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing, software
    # distributed under the License is distributed on an "AS IS" BASIS,
    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    # See the License for the specific language governing permissions and
    # limitations under the License.
    #

    # This file is sourced when running various Spark programs.
    # Copy it as spark-env.sh and edit that to configure Spark for your site.

    # Options read when launching programs locally with
    # ./bin/run-example or ./bin/spark-submit
    # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
    # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
    # - SPARK_PUBLIC_DNS, to set the public dns name of the driver program

    # Options read by executors and drivers running inside the cluster
    # - SPARK_LOCAL_IP, to set the IP address Spark binds to on this node
    # - SPARK_PUBLIC_DNS, to set the public DNS name of the driver program
    # - SPARK_LOCAL_DIRS, storage directories to use on this node for shuffle and RDD data
    # - MESOS_NATIVE_JAVA_LIBRARY, to point to your libmesos.so if you use Mesos

    # Options read in YARN client/cluster mode
    # - SPARK_CONF_DIR, Alternate conf dir. (Default: ${SPARK_HOME}/conf)
    # - HADOOP_CONF_DIR, to point Spark towards Hadoop configuration files
    # - YARN_CONF_DIR, to point Spark towards YARN configuration files when you use YARN
    # - SPARK_EXECUTOR_CORES, Number of cores for the executors (Default: 1).
    # - SPARK_EXECUTOR_MEMORY, Memory per Executor (e.g. 1000M, 2G) (Default: 1G)
    # - SPARK_DRIVER_MEMORY, Memory for Driver (e.g. 1000M, 2G) (Default: 1G)

    # Options for the daemons used in the standalone deploy mode
    # - SPARK_MASTER_HOST, to bind the master to a different IP address or hostname
    # - SPARK_MASTER_PORT / SPARK_MASTER_WEBUI_PORT, to use non-default ports for the master
    # - SPARK_MASTER_OPTS, to set config properties only for the master (e.g. "-Dx=y")
    # - SPARK_WORKER_CORES, to set the number of cores to use on this machine
    # - SPARK_WORKER_MEMORY, to set how much total memory workers have to give executors (e.g. 1000m, 2g)
    # - SPARK_WORKER_PORT / SPARK_WORKER_WEBUI_PORT, to use non-default ports for the worker
    # - SPARK_WORKER_DIR, to set the working directory of worker processes
    # - SPARK_WORKER_OPTS, to set config properties only for the worker (e.g. "-Dx=y")
    # - SPARK_DAEMON_MEMORY, to allocate to the master, worker and history server themselves (default: 1g).
    # - SPARK_HISTORY_OPTS, to set config properties only for the history server (e.g. "-Dx=y")
    # - SPARK_SHUFFLE_OPTS, to set config properties only for the external shuffle service (e.g. "-Dx=y")
    # - SPARK_DAEMON_JAVA_OPTS, to set config properties for all daemons (e.g. "-Dx=y")
    # - SPARK_DAEMON_CLASSPATH, to set the classpath for all daemons
    # - SPARK_PUBLIC_DNS, to set the public dns name of the master or workers

    # Generic options for the daemons used in the standalone deploy mode
    # - SPARK_CONF_DIR      Alternate conf dir. (Default: ${SPARK_HOME}/conf)
    # - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME}/logs)
    # - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)
    # - SPARK_IDENT_STRING  A string representing this instance of spark. (Default: $USER)
    # - SPARK_NICENESS      The scheduling priority for daemons. (Default: 0)
    # - SPARK_NO_DAEMONIZE  Run the proposed command in the foreground. It will not output a PID file.
    # Options for native BLAS, like Intel MKL, OpenBLAS, and so on.
    # You might get better performance to enable these options if using native BLAS (see SPARK-21305).
    # - MKL_NUM_THREADS=1        Disable multi-threading of Intel MKL
    # - OPENBLAS_NUM_THREADS=1   Disable multi-threading of OpenBLAS


    export JAVA_HOME=/user/apps/jdk1.8.0_131
    export SCALA_HOME=/user/apps/scala-2.11.8
    export HADOOP_HOME=/user/apps/hadoop-2.7.1
    export HADOOP_CONF_DIR=/user/apps/hadoop-2.7.1/etc/hadoop
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/yarn-site.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: yarn-site
  namespace: default
data:
  yarn-site-config: |-
    <?xml version="1.0"?>
    <!--
      Licensed under the Apache License, Version 2.0 (the "License");
      you may not use this file except in compliance with the License.
      You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License. See accompanying LICENSE file.
    -->
    <configuration>
      <property>  
          <name>yarn.nodemanager.vmem-check-enabled</name>  
          <value>false</value>  
      </property> 
      <property>
              <name>yarn.resourcemanager.ha.enabled</name>
              <value>true</value>
      </property>

      <property>
              <name>yarn.resourcemanager.recovery.enabled</name>
              <value>true</value>
      </property>

      <property>
              <name>yarn.resourcemanager.connect.retry-interval.ms</name>
              <value>2000</value>
      </property>

      <property>
              <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
              <value>true</value>
      </property>

        <property>
            <name>yarn.resourcemanager.cluster-id</name>
            <value>yrc</value>
      </property>

      <property>
            <name>yarn.resourcemanager.ha.rm-ids</name>
            <value>rm1,rm2</value>
      </property>

        <property>
            <name>yarn.resourcemanager.hostname.rm1</name>
            <value>apm-hadoop-ss-1.ha-svc.default.svc.cluster.local</value>
        </property>

        <property>
            <name>yarn.resourcemanager.hostname.rm2</name>
            <value>apm-hadoop-ss-2.ha-svc.default.svc.cluster.local</value>
      </property>

        <property>
            <name>yarn.resourcemanager.webapp.address.rm1</name>
            <value>apm-hadoop-ss-1.ha-svc.default.svc.cluster.local:8088</value>
        </property>

        <property>
            <name>yarn.resourcemanager.webapp.address.rm2</name>
            <value>apm-hadoop-ss-2.ha-svc.default.svc.cluster.local:8088</value>
        </property>

          <property>
            <name>yarn.resourcemanager.zk-address</name>
            <value>apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:2181,apm-hadoop-ss-1.ha-svc.default.svc.cluster.local:2181,apm-hadoop-ss-2.ha-svc.default.svc.cluster.local:2181</value>
        </property>

          <property>    
            <name>yarn.nodemanager.aux-services</name>    
            <value>mapreduce_shuffle</value>    
        </property> 

        <property>
            <name>yarn.nodemanager.pmem-check-enabled</name>
            <value>false</value>
        </property>
        <property>
            <name>yarn.nodemanager.vmem-check-enabled</name>
            <value>false</value>
        </property>
        <property>
                <name>yarn.log-aggregation-enable</name>
                <value>true</value>
        </property>

        <property>
                <name>yarn.log-aggregation.retain-seconds</name>
                <value>86400</value>
        </property>

      <property>
              <name>yarn.log.server.url</name>
              <value>http://apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:19888/jobhistory/logs/</value>
        </property>

        <property>
            <name>yarn.log-aggregation.retain-check-interval-seconds</name>
            <value>86400</value>
        </property> 

        
    </configuration>
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/zoocfg.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: zoocfg
  namespace: default
data:
  zoocfg-config: |-
    # The number of milliseconds of each tick
    tickTime=2000
    # The number of ticks that the initial 
    # synchronization phase can take
    initLimit=10
    # The number of ticks that can pass between 
    # sending a request and getting an acknowledgement
    syncLimit=5
    # the directory where the snapshot is stored.
    # do not use /tmp for storage, /tmp here is just 
    # example sakes.
    dataDir=/user/data/zookeeper/data
    # the port at which the clients will connect
    clientPort=2181
    # the maximum number of client connections.
    # increase this if you need to handle more clients
    #maxClientCnxns=60
    #
    # Be sure to read the maintenance section of the 
    # administrator guide before turning on autopurge.
    #
    # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
    #
    # The number of snapshots to retain in dataDir
    #autopurge.snapRetainCount=3
    # Purge task interval in hours
    # Set to "0" to disable auto purge feature
    #autopurge.purgeInterval=1
    server.0=apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:2888:3888
    server.1=apm-hadoop-ss-1.ha-svc.default.svc.cluster.local:2888:3888
    server.2=apm-hadoop-ss-2.ha-svc.default.svc.cluster.local:2888:3888
---
# Source: apm-zhice-api-java/charts/apm-mysql/templates/mysqld.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: apm-mysql-mysqldcnf
  namespace: default
data:
  mysqldcnf-config: |-
    # java-mysql Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
    #
    # This program is free software; you can redistribute it and/or modify
    # it under the terms of the GNU General Public License as published by
    # the Free Software Foundation; version 2 of the License.
    #
    # This program is distributed in the hope that it will be useful,
    # but WITHOUT ANY WARRANTY; without even the implied warranty of
    # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    # GNU General Public License for more details.
    #
    # You should have received a copy of the GNU General Public License
    # along with this program; if not, write to the Free Software
    # Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301 USA

    #
    # The MySQL  Server configuration file.
    #
    # For explanations see
    # http://dev.mysql.com/doc/mysql/en/server-system-variables.html

    [mysqld]
    pid-file        = /var/run/mysqld/mysqld.pid
    socket          = /var/run/mysqld/mysqld.sock
    datadir         = /var/lib/mysql
    #log-error      = /var/log/mysql/error.log
    # By default we only accept connections from localhost
    #bind-address   = 127.0.0.1
    # Disabling symbolic-links is recommended to prevent assorted security risks
    symbolic-links=0
    character_set_server=utf8mb4
    explicit_defaults_for_timestamp=true
    max_connections = 5000
---
# Source: apm-zhice-api-java/templates/javaapi-application.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: apm-zhice-api-java-application-properties-config
  namespace: default
data:
  application-properties: |-
    #244使用
    spring.datasource.primary.jdbc-url = jdbc:mysql://apm-mysql-svc:3306/lens_dataprocess_plat?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true
    spring.datasource.primary.username = root
    spring.datasource.primary.password = pw123456
    spring.datasource.primary.driver-class-name = com.mysql.cj.jdbc.Driver
    spring.datasource.secondary.jdbc-url=jdbc:mysql://apm-mysql-svc:3306/studio?useUnicode=true&characterEncoding=utf8&useSSL=false&autoReconnect=true
    spring.datasource.secondary.username=root
    spring.datasource.secondary.password=pw123456
    spring.datasource.secondary.driver-class-name=com.mysql.cj.jdbc.Driver

    # Specify the DBMS
    spring.jpa.database = MYSQL
    # Show or not log for each sql query
    spring.jpa.show-sql = true

    #上传数据spring.datasource.dbcp2.max-idle = 100
    spring.http.multipart.max-file-size=1000Mb
    spring.http.multipart.max-request-size=1000Mb

    server.maxHttpHeaderSize=102400000
    server.maxHttpPostSize =102400000
    # Single file max size
    multipart.maxFileSize=500Mb

    # All files max size
    multipart.maxRequestSize=500Mb
---
# Source: apm-zhice-api-java/templates/javaapi-server-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: apm-zhice-api-java-server-config
  namespace: default
data:
  server-config: |-
    <?xml version="1.0"?>
    <config>
      <!--本地所需配置-->
      <!--http://web-0.ha-svc.elens-zhitu.svc.cluster.local:11000/oozie-->
      <OOZIE_CLIENT>http://apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:11000/oozie</OOZIE_CLIENT>
      <NAME_NODE>hdfs:/apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:9000,hdfs://apm-hadoop-ss-2.ha-svc.default.svc.cluster.local:9000</NAME_NODE>
      <HA_NAME_NODE>hdfs://apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:9000,hdfs://apm-hadoop-ss-2.ha-svc.default.svc.cluster.local:9000</HA_NAME_NODE>
      <YARN_ADDRESS>http://apm-hadoop-ss-1.ha-svc.default.svc.cluster.local:8088</YARN_ADDRESS>
      <YARN_LOGSADDRESS>apm-hadoop-ss-0.ha-svc.default.svc.cluster.local:8042,apm-hadoop-ss-1.ha-svc.default.svc.cluster.local:8042,apm-hadoop-ss-2.ha-svc.default.svc.cluster.local:8042</YARN_LOGSADDRESS>
      <JOB_TRACKER>apm-hadoop-ss-1.ha-svc.default.svc.cluster.local:8032</JOB_TRACKER>

      <HOSTNAME_CON>true</HOSTNAME_CON>
      <QUEUE_NAME>default</QUEUE_NAME>
      <APP_WORKSPACE>/EML/oozie</APP_WORKSPACE>
      <DRAFT_PATH>/EML/draft</DRAFT_PATH>
      <DATASET_PATH>/EML/lensData</DATASET_PATH>
      <DOWNLOAD_PATH>src/main/webapp/dowload/</DOWNLOAD_PATH>
      <MODULE_PATH>/EML/Modules</MODULE_PATH>
      <PROGRAMS_PATH>/EML/Programs</PROGRAMS_PATH>

      <DB_HOST>apm-mysql-svc</DB_HOST>
      <DB_PORT>3306</DB_PORT>
      <DB_NAME>lens_dataprocess_plat</DB_NAME>
      <DB_USER>root</DB_USER>
      <DB_PASSWORD>pw123456</DB_PASSWORD>
      <DB_TIMEOUT>100</DB_TIMEOUT>

      <MAIL_HOST>mail.software.ict.ac.cn</MAIL_HOST>
      <MAIL_USERNAME>bda@software.ict.ac.cn</MAIL_USERNAME>
      <MAIL_PASSWORD>bda@ict</MAIL_PASSWORD>
      <TENSORFLOW_CLUSTER>elens-server2, elens-server3, elens-server5</TENSORFLOW_CLUSTER>
      <TENSORFLOW_MASTER>elens-server2</TENSORFLOW_MASTER>
      <TENSORFLOW_USER>root</TENSORFLOW_USER>

      <!--ha模式新增nameservices配置-->
      <NAME_SERVICES>ns</NAME_SERVICES>
      <HA_NAME_NODES>nn1,nn2</HA_NAME_NODES>

    </config>
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-mysql/templates/vol.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: apm-mysql-sc-default
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-redis/charts/apm-mysql/templates/vol.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: apm-mysql-sc-default
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/vol.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: apm-hadoop-sc-default-0
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/vol.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: apm-hadoop-sc-default-2
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/vol.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: apm-hadoop-sc-default-1
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
# Source: apm-zhice-api-java/charts/apm-mysql/templates/vol.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: apm-mysql-sc-default
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-mysql/templates/vol.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: apm-mysql-sc-default
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: apm-mysql-sc-default
  nfs:
    path: /nfsdata/default/apm-mysql
    server: 192.168.1.163
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-redis/charts/apm-mysql/templates/vol.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: apm-mysql-sc-default
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: apm-mysql-sc-default
  nfs:
    path: /nfsdata/default/apm-mysql
    server: 192.168.1.163
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/vol.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: apm-hadoop-pv-default-1
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: apm-hadoop-sc-default-1
  nfs:
    path: /nfsdata/default/apm-hadoop-apm-hadoop-ss-1


    server: 192.168.1.163
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/vol.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: apm-hadoop-pv-default-2
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: apm-hadoop-sc-default-2
  nfs:
    path: /nfsdata/default/apm-hadoop-apm-hadoop-ss-2


    server: 192.168.1.163
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/vol.yaml
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: apm-hadoop-pv-default-0
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: apm-hadoop-sc-default-0
  nfs:
    path: /nfsdata/default/apm-hadoop-apm-hadoop-ss-0


    server: 192.168.1.163
---
# Source: apm-zhice-api-java/charts/apm-mysql/templates/vol.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: apm-mysql-sc-default
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: apm-mysql-sc-default
  nfs:
    path: /nfsdata/default/apm-mysql
    server: 192.168.1.163
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-mysql/templates/vol.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: apm-mysql-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: apm-mysql-sc-default
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-redis/charts/apm-mysql/templates/vol.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: apm-mysql-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: apm-mysql-sc-default
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/vol.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: apm-hadoop-apm-hadoop-ss-1
  namespace: default
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: apm-hadoop-sc-default-1
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/vol.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: apm-hadoop-apm-hadoop-ss-2
  namespace: default
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: apm-hadoop-sc-default-2
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/vol.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: apm-hadoop-apm-hadoop-ss-0
  namespace: default
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  storageClassName: apm-hadoop-sc-default-0
---
# Source: apm-zhice-api-java/charts/apm-mysql/templates/vol.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: apm-mysql-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: apm-mysql-sc-default
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-mysql/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: apm-mysql-svc
  namespace: default
  labels:
    app: apm-mysql-svc
spec:
  type: NodePort
  ports:
    - port: 3306
      targetPort: mysql-port
      protocol: TCP
      name: http
  selector:
    app: apm-mysql-pod
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-redis/charts/apm-mysql/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: apm-mysql-svc
  namespace: default
  labels:
    app: apm-mysql-svc
spec:
  type: NodePort
  ports:
    - port: 3306
      targetPort: mysql-port
      protocol: TCP
      name: http
  selector:
    app: apm-mysql-pod
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-redis/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: apm-redis-svc
  labels:
    dep: apm-redis-svc
spec:
  type: NodePort
  ports:
    - port: 6379
      targetPort: redis-port
  selector:
    app: apm-redis-redis
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/browser.yaml
apiVersion: v1
kind: Service
metadata:
  name: hdfs-browser1
  labels:
    dep: hdfs-browser
spec:
  type: NodePort
  ports:
    - name: hdfs
      port: 50070
      targetPort: 50070
  selector:
    statefulset.kubernetes.io/pod-name: apm-hadoop-ss-2
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/browser.yaml
apiVersion: v1
kind: Service
metadata:
  name: history-browser
  labels:
    dep: history-browser
spec:
  type: NodePort
  ports:
    - name: history
      port: 19888
      targetPort: 19888
  selector:
    statefulset.kubernetes.io/pod-name: apm-hadoop-ss-0
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/browser.yaml
apiVersion: v1
kind: Service
metadata:
  name: yarn-browser1
  labels:
    dep: yarn-browser
spec:
  type: NodePort
  ports:
    - name: yarn
      port: 8088
      targetPort: 8088
  selector:
    statefulset.kubernetes.io/pod-name: apm-hadoop-ss-1
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/browser.yaml
apiVersion: v1
kind: Service
metadata:
  name: oozie-browser
  labels:
    dep: oozie-browser
spec:
  type: NodePort
  ports:
    - name: oozie
      port: 11000
      targetPort: 11000
  selector:
    statefulset.kubernetes.io/pod-name: apm-hadoop-ss-0
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/browser.yaml
apiVersion: v1
kind: Service
metadata:
  name: yarn-browser2
  labels:
    dep: yarn-browser
spec:
  type: NodePort
  ports:
    - name: yarn
      port: 8088
      targetPort: 8088
  selector:
    statefulset.kubernetes.io/pod-name: apm-hadoop-ss-2
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/browser.yaml
apiVersion: v1
kind: Service
metadata:
  name: hdfs-browser0
  labels:
    dep: hdfs-browser
spec:
  type: NodePort
  ports:
    - name: hdfs
      port: 50070
      targetPort: 50070
  selector:
    statefulset.kubernetes.io/pod-name: apm-hadoop-ss-0
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/browser.yaml
apiVersion: v1
kind: Service
metadata:
  name: spark-browser
  labels:
    dep: spark-browser
spec:
  type: NodePort
  ports:
    - name: sparkui
      port: 7180
      targetPort: 7180
    - name: sparkhis1
      port: 18080
      targetPort: 18080
    - name: sparkhis2
      port: 18089
      targetPort: 18089
  selector:
    statefulset.kubernetes.io/pod-name: apm-hadoop-ss-2
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/hadoop-ss.yaml
kind: Service
apiVersion: v1
metadata:
  name: ha-svc
  lables:
    app: ha-svc
spec:
  ports:
  - name: ss-port
    port: 22
  selector:
    app: centospod
  clusterIP: None
---
# Source: apm-zhice-api-java/charts/apm-mysql/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: apm-mysql-svc
  namespace: default
  labels:
    app: apm-mysql-svc
spec:
  type: NodePort
  ports:
    - port: 3306
      targetPort: mysql-port
      protocol: TCP
      name: http
  selector:
    app: apm-mysql-pod
---
# Source: apm-zhice-api-java/templates/javaapi-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: apm-zhice-api-java-svc
  labels:
    svc: apm-zhice-api-java-svc
spec:
  type: NodePort
  ports:
  - name: p8080
    port: 8080
  selector:
    app: apm-zhice-api-java-pod
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/init-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: apm-hadoop-init
  labels:
    app: apm-hadoop-init
spec:
  restartPolicy: OnFailure
  containers:
  - name: init
    image: registry.elensdata.com/apm/zhidatabase:1.6.0
    imagePullPolicy: IfNotPresent
    command: ["/bin/bash", "-c", "sh /user/init.sh"]
    volumeMounts:
    - name: sh
      mountPath: /user/init.sh
      subPath: init.sh
    # - name: vl
    #   mountPath: /initdata
  volumes:
  - name: sh
    configMap:
      name: initpsh
      items:
      - key: initpsh-config
        path: init.sh
  # - name: vl
  #   persistentVolumeClaim:
  #     claimName: init-vl-c
    
  tolerations:
  - key: "key"
    operator: "Equal"
    value: "test02"
    effect: "PreferNoSchedule"
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-mysql/templates/dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apm-mysql-dep
  namespace: default
  labels:
    app: apm-mysql-dep
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apm-mysql-pod
  template:
    metadata:
      labels:
        app: apm-mysql-pod
    spec:
      containers:
        - name: con
          image: registry.elensdata.com/apm/mysql:5.7.28
          imagePullPolicy: IfNotPresent
          env:
            - name: "MYSQL_ROOT_PASSWORD"
              value: "pw123456"
            - name: "test_env"
              value: "hadoop-mysql-test"
            - name: "test_tt"
              value: "mysql-tt123"
          ports:
            - name: mysql-port
              containerPort: 3306
          volumeMounts:
            - mountPath: "/var/lib/mysql"
              name: mysqldb
            - name: mysqldcnf
              mountPath: /etc/mysql/mysql.conf.d/mysqld.cnf
              subPath: mysqld.cnf
      volumes:
        - name: mysqldb
          persistentVolumeClaim:
            claimName: apm-mysql-pvc
        - name: mysqldcnf
          configMap:
            name: apm-mysql-mysqldcnf
            items:
              - key: mysqldcnf-config
                path: mysqld.cnf
      # tolerations:
      #   - key: "key"
      #     operator: "Equal"
      #     value: "test02"
      #     effect: "PreferNoSchedule"
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-redis/charts/apm-mysql/templates/dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apm-mysql-dep
  namespace: default
  labels:
    app: apm-mysql-dep
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apm-mysql-pod
  template:
    metadata:
      labels:
        app: apm-mysql-pod
    spec:
      containers:
        - name: con
          image: registry.elensdata.com/apm/mysql:5.7.28
          imagePullPolicy: IfNotPresent
          env:
            - name: "MYSQL_ROOT_PASSWORD"
              value: "pw123456"
            - name: "test_env"
              value: "java-hadoop-mysql-test"
            - name: "test_tt"
              value: "java-hadoop-mysql-tt"
          ports:
            - name: mysql-port
              containerPort: 3306
          volumeMounts:
            - mountPath: "/var/lib/mysql"
              name: mysqldb
            - name: mysqldcnf
              mountPath: /etc/mysql/mysql.conf.d/mysqld.cnf
              subPath: mysqld.cnf
      volumes:
        - name: mysqldb
          persistentVolumeClaim:
            claimName: apm-mysql-pvc
        - name: mysqldcnf
          configMap:
            name: apm-mysql-mysqldcnf
            items:
              - key: mysqldcnf-config
                path: mysqld.cnf
      # tolerations:
      #   - key: "key"
      #     operator: "Equal"
      #     value: "test02"
      #     effect: "PreferNoSchedule"
---
# Source: apm-zhice-api-java/charts/apm-hadoop/charts/apm-redis/templates/deployment.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: apm-redis-redis
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: apm-redis-redis
    spec:
      containers:
      - name: con
        image: registry.elensdata.com/apm/redis:5.0.4
        imagePullPolicy: IfNotPresent
        ports:
        - name: redis-port
          containerPort: 6379
      # tolerations:
      # - key: "key"
      #   operator: "Equal"
      #   value: "test02"
      #   effect: "PreferNoSchedule"
---
# Source: apm-zhice-api-java/charts/apm-mysql/templates/dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apm-mysql-dep
  namespace: default
  labels:
    app: apm-mysql-dep
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apm-mysql-pod
  template:
    metadata:
      labels:
        app: apm-mysql-pod
    spec:
      containers:
        - name: con
          image: registry.elensdata.com/apm/mysql:5.7.28
          imagePullPolicy: IfNotPresent
          env:
            - name: "MYSQL_ROOT_PASSWORD"
              value: "pw123456"
            - name: "test_env"
              value: "java-mysql"
            - name: "test_tt"
              value: "mysql-tt123"
          ports:
            - name: mysql-port
              containerPort: 3306
          volumeMounts:
            - mountPath: "/var/lib/mysql"
              name: mysqldb
            - name: mysqldcnf
              mountPath: /etc/mysql/mysql.conf.d/mysqld.cnf
              subPath: mysqld.cnf
      volumes:
        - name: mysqldb
          persistentVolumeClaim:
            claimName: apm-mysql-pvc
        - name: mysqldcnf
          configMap:
            name: apm-mysql-mysqldcnf
            items:
              - key: mysqldcnf-config
                path: mysqld.cnf
      # tolerations:
      #   - key: "key"
      #     operator: "Equal"
      #     value: "test02"
      #     effect: "PreferNoSchedule"
---
# Source: apm-zhice-api-java/templates/javaapi-dep.yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: apm-zhice-api-java-dep
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: apm-zhice-api-java-pod
    spec:
      containers:
      - name: con
        image: registry.elensdata.com/zhice-apm/java:1.0.1
        imagePullPolicy: IfNotPresent
        # command: ["/bin/bash", "-c", "tail -f /etc/hosts"]
        volumeMounts:
        - mountPath: /user/apps/tomcat/webapps/lensData/WEB-INF/classes/application.properties
          name: application-properties-v
          subPath: application.properties
        - name: server-config
          mountPath: /user/apps/tomcat/webapps/lensData/WEB-INF/classes/server-config.xml
          subPath: server-config.xml
        ports:
        - name: javaport
          containerPort: 8080
      volumes:
      - name: application-properties-v
        configMap:
          name: apm-zhice-api-java-application-properties-config
          items: 
          - key: application-properties
            path: application.properties
      - name: server-config
        configMap:
          name: apm-zhice-api-java-server-config
          items:
          - key: server-config
            path: server-config.xml

      # tolerations:
      # - key: "key"
      #   operator: "Equal"
      #   value: "test02"
      #   effect: "PreferNoSchedule"
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/hadoop-ss.yaml
kind: StatefulSet
apiVersion: apps/v1
metadata:
  name: apm-hadoop-ss
spec:
  selector:
    matchLabels:
      app: centospod
  serviceName: ha-svc
  replicas: 3
  template:
    metadata:
      labels:
        app: centospod
    spec:
      containers:
      - name: cc
        image: registry.elensdata.com/apm/zhidatabase:1.6.0
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c", "sh /user/init.sh"]
        securityContext:
          privileged: true
        volumeMounts:
        - name: core-site
          mountPath: /user/apps/hadoop-2.7.1/etc/hadoop/core-site.xml
          subPath: core-site.xml
        - name: core-site
          mountPath: /user/apps/hbase-1.4.4/conf/core-site.xml 
          subPath: core-site.xml
        - name: hdfs-site
          mountPath: /user/apps/hadoop-2.7.1/etc/hadoop/hdfs-site.xml
          subPath: hdfs-site.xml
        - name: hdfs-site
          mountPath: /user/apps/hbase-1.4.4/conf/hdfs-site.xml
          subPath: hdfs-site.xml
        - name: mapred-site
          mountPath: /user/apps/hadoop-2.7.1/etc/hadoop/mapred-site.xml
          subPath: mapred-site.xml
        - name: yarn-site
          mountPath: /user/apps/hadoop-2.7.1/etc/hadoop/yarn-site.xml
          subPath: yarn-site.xml
        - name: slaves
          mountPath: /user/apps/hadoop-2.7.1/etc/hadoop/slaves
          subPath: slaves
        - name: sparkenv
          mountPath: /user/apps/spark-2.3.4/conf/spark-env.sh
          subPath: spark-env.sh
        - name: zoocfg
          mountPath: /user/apps/zookeeper-3.4.12/conf/zoo.cfg
          subPath: zoo.cfg
        - name: hadoop-env
          mountPath: /user/apps/hadoop-2.7.1/etc/hadoop/hadoop-env.sh
          subPath: hadoop-env.sh
        - name: hive-site
          mountPath: /user/apps/hive-1.2.1/conf/hive-site.xml
          subPath: hive-site.xml
        - name: hive-log4j
          mountPath: /user/apps/hive-1.2.1/conf/hive-log4j.properties 
          subPath: hive-log4j.properties 
        - name: hive-env
          mountPath: /user/apps/hive-1.2.1/conf/hive-env.sh
          subPath: hive-env.sh
        - name: hbase-env
          mountPath: /user/apps/hbase-1.4.4/conf/hbase-env.sh
          subPath: hbase-env.sh
        - name: hbase-site
          mountPath: /user/apps/hbase-1.4.4/conf/hbase-site.xml
          subPath: hbase-site.xml
        - name: oozie-site
          mountPath: /user/apps/oozie-4.0.0/conf/oozie-site.xml
          subPath: oozie-site.xml
        - name: sh
          mountPath: /user/init.sh
          subPath: init.sh
        - name: apm-hadoop
          mountPath: /user/data/hadoop
      volumes:
      - name: core-site
        configMap:
          name: core-site
          items:
          - key: core-site-config
            path: core-site.xml
      - name: hdfs-site
        configMap:
          name: hdfs-site
          items:
          - key: hdfs-site-config
            path: hdfs-site.xml
      - name: mapred-site
        configMap:
          name: mapred-site
          items:
          - key: mapred-site-config
            path: mapred-site.xml
      - name: yarn-site
        configMap:
          name: yarn-site
          items:
          - key: yarn-site-config
            path: yarn-site.xml
      - name: slaves
        configMap:
          name: slaves
          items:
          - key: slaves-config
            path: slaves
      - name: sparkenv
        configMap:
          name: sparkenv
          items:
          - key: sparkenv-config
            path: spark-env.sh
      - name: zoocfg
        configMap:
          name: zoocfg
          items:
          - key: zoocfg-config
            path: zoo.cfg
      - name: hadoop-env
        configMap:
          name: hadoop-env
          items:
          - key: hadoop-env-config
            path: hadoop-env.sh
      - name: hive-site
        configMap:
          name: hive-site
          items:
          - key: hive-site-config
            path: hive-site.xml
      - name: hive-log4j
        configMap:
          name: hive-log4j
          items:
          - key: hive-log4j-config
            path: hive-log4j.properties
      - name: hive-env
        configMap:
          name: hive-env
          items:
          - key: hive-env-config
            path: hive-env.sh
      - name: hbase-env
        configMap:
          name: hbase-env
          items:
          - key: hbase-env-config
            path: hbase-env.sh
      - name: hbase-site
        configMap:
          name: hbase-site
          items:
          - key: hbase-site-config
            path: hbase-site.xml
      - name: oozie-site
        configMap:
          name: oozie-site
          items:
          - key: oozie-site-config
            path: oozie-site.xml
      - name: sh
        configMap:
          name: initsh
          items:
          - key: initsh-config
            path: init.sh
      tolerations:
      - key: "key"
        operator: "Equal"
        value: "test02"
        effect: "PreferNoSchedule"

  volumeClaimTemplates:
  - metadata:
      name: apm-hadoop
    spec:
      accessModes:
      - "ReadWriteOnce"
      resources:
        requests:
          storage: 1Gi
---
# Source: apm-zhice-api-java/charts/apm-hadoop/templates/vol.yaml
# ---

# kind: StorageClass
# apiVersion: storage.k8s.io/v1
# metadata:
#   name: init-vl-s
# provisioner: kubernetes.io/no-provisioner
# volumeBindingMode: WaitForFirstConsumer

# --- 

# apiVersion: v1
# kind: PersistentVolume
# metadata:
#   name: init-vl-pv
# spec:
#   capacity:
#     storage: 10M
#   volumeMode: Filesystem 
#   accessModes:
#   - ReadWriteOnce
#   persistentVolumeReclaimPolicy: Retain
#   storageClassName: init-vl-s
#   nfs: 
#     path: /nfsdata/elensAIO/zhitu/init
#     server: 192.168.1.163

# ---

# apiVersion: v1
# kind: PersistentVolumeClaim
# metadata:
#   name: init-vl-c
# spec:
#   accessModes:
#   - ReadWriteOnce
#   resources:
#     requests:
#       storage: 10M
#   storageClassName: init-vl-s
